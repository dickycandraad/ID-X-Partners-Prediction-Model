# -*- coding: utf-8 -*-
"""Prediction Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjZ6OZJAnWQpgccAsS3CxddEBSlQFgpa

# **PREDICTION MODEL**

# Data Understanding
"""

# Menghubungkan environment Colab dengan Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Memuat dataset
import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/loan_data_2007_2014.csv")

# Menampilkan lima baris teratas dari data
data.head()

# Menampilkan informasi data
data.info()

# Mengecek jumlah NaN di setiap kolom
null_columns = data.isnull().sum()
null_columns[null_columns > 0]

# Menampilkan statisik deskriptif dari data
data.describe()

# Mengecek duplikasi data
if data['id'].nunique() == data['member_id'].nunique():
    print("Setiap baris mewakili individu yang unik")
else:
    print("Terdapat duplikasi pada data")

"""# Feature Engineering

### Defining Target Variable
"""

# Menghapus kolom yang tidak digunakan

# Mendefinisikan kolom yang akan dihapus
columns_to_drop = [
    'Unnamed: 0',
    'id',
    'member_id',
    'url',
    'desc',
    'zip_code',
    'annual_inc_joint',
    'dti_joint',
    'verification_status_joint',
    'open_acc_6m',
    'open_il_6m',
    'open_il_12m',
    'open_il_24m',
    'mths_since_rcnt_il',
    'total_bal_il',
    'il_util',
    'open_rv_12m',
    'open_rv_24m',
    'max_bal_bc',
    'all_util',
    'inq_fi',
    'total_cu_tl',
    'inq_last_12m',
    'mths_since_last_major_derog',
    'tot_coll_amt',
    'tot_cur_bal',
    'total_rev_hi_lim',
    'sub_grade'
]

# Menghapus kolom dari dataset
data.drop(columns=columns_to_drop, axis=1, inplace=True)

# Menampilkan informasi data setelah penghapusan kolom
data.info()

# Menampilkan statistik deskriptif setelah penghapusan kolom
data.describe()

# Proporsi nilai pada kolom 'loan_status'
percentage_loan_status = data.loan_status.value_counts(normalize=True) * 100
percentage_loan_status

# Daftar status pembayaran yang dianggap sebagai 'bad_status'
bad_status = [
    'Charged Off',
    'Default',
    'Does not meet the credit policy. Status:Charged Off',
    'Late (31-120 days)'
]

# Membuat kolom target biner 'bad_flag' (1 = bad, 0 = good)
data['bad_flag'] = data['loan_status'].isin(bad_status).astype(int)

# Menghitung persentase distribusi nilai dalam kolom 'bad_flag'
percentage_bad_flag = data['bad_flag'].value_counts(normalize=True) * 100
percentage_bad_flag

# Menghapus kolom 'loan_status'
data.drop('loan_status', axis=1, inplace=True)

"""### Data Cleaning

##### Column: emp_length
"""

# Menampilkan nilai unik dari kolom 'emp_length'
data['emp_length'].unique()

# Membersihkan dan mengubah tipe data menjadi numerik
data['emp_length_int'] = pd.to_numeric(data['emp_length'].str.replace(r'\D', '', regex=True), errors='coerce')

# Menampilkan lima baris teratas dari kolom 'emp_length_int'
data['emp_length_int'].head()

# Menghapus kolom 'emp_length'
data.drop('emp_length', axis=1, inplace=True)

"""##### Column: term"""

# Menampilkan nilai unik dari kolom 'term'
unique_terms = data['term'].unique()
unique_terms

# Membersihkan dan mengubah tipe data menjadi numerik
data['term_int'] = data['term'].str.replace(' months', '').astype(int)

# Menampilkan lima baris teratas dari kolom 'term_int'
data['term_int'].head()

# Menghapus kolom 'term'
data.drop('term', axis=1, inplace=True)

"""##### Column: earlist_cr_line"""

# Menampilkan lima baris teratas dari kolom 'earliest_cr_line'
data['earliest_cr_line'].head()

# Mengubah kolom 'earliest_cr_line' menjadi format tanggal
data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line'], format = '%b-%y')

# Menampilkan lima baris teratas dari kolom 'earliest_cr_line_date'
data['earliest_cr_line_date'].head()

# Menghitung jumlah bulan sejak 'earliest_cr_line' hingga tanggal referensi
data['mths_since_earliest_cr_line'] = round((pd.to_datetime('2017-12-01') - data['earliest_cr_line_date']).dt.days / 30.5)
data['mths_since_earliest_cr_line'].head()

# Menampilkan statistik deskriptif dari kolom 'mths_since_earliest_cr_line'
data['mths_since_earliest_cr_line'].describe()

# Menampilkan baris dengan nilai 'mths_since_earliest_cr_line' yang kurang dari 0
data[data['mths_since_earliest_cr_line'] < 0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head()

# Mengganti nilai 'mths_since_earliest_cr_line' yang bernilai kurang dari 0 dengan nilai maksimum
data.loc[data['mths_since_earliest_cr_line'] < 0, 'mths_since_earliest_cr_line'] = data['mths_since_earliest_cr_line'].max()

# Menghapus kolom 'earliest_cr_line' dan 'earliest_cr_line_date'
data.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""##### Column: issue_d"""

# Mengubah kolom 'issue_d' menjadi format tanggal
data['issue_d_date'] = pd.to_datetime(data['issue_d'], format = '%b-%y')

# Menghitung jumlah bulan sejak 'issue_d' hingga tanggal referensi
data['mths_since_issue_d'] = round((pd.to_datetime('2017-12-01') - data['issue_d_date']).dt.days / 30.5)

# Menampilkan lima baris teratas dari kolom 'mths_since_issue_d'
data['mths_since_issue_d'].head()

# Menampilkan statistik deskriptif dari kolom 'mths_since_issue_d'
data['mths_since_issue_d'].describe()

# Menghapus kolom 'issue_d' dan 'issue_d_date'
data.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""##### Column: last_pymnt_d"""

# Mengubah kolom 'last_pymnt_d_date' menjadi format tanggal
data['last_pymnt_d_date'] = pd.to_datetime(data['last_pymnt_d'], format = '%b-%y')

# Menghitung jumlah bulan sejak 'last_pymnt_d' hinggal tanggal referensi
data['mths_since_last_pymnt_d'] = round((pd.to_datetime('2017-12-01') - data['last_pymnt_d_date']).dt.days / 30.5)

# Menampilkan lima baris teratas dari kolom 'mths_since_last_pymnt_d'
data['mths_since_last_pymnt_d'].head()

# Menampilkan statistik deskriptif dari kolom 'mths_since_last_pymnt_d'
data['mths_since_last_pymnt_d'].describe()

# Menghapus kolom 'last_pymnt_d' dan 'last_pymnt_d_date'
data.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""##### Column: next_pymnt_d"""

# Mengubah kolom 'next_pymnt_d' menjadi format tanggal
data['next_pymnt_d_date'] = pd.to_datetime(data['next_pymnt_d'], format = '%b-%y')

# Menghitung jumlah bulan sejak 'next_pymnt_d' hingga tanggal referensi
data['mths_since_next_pymnt_d'] = round((pd.to_datetime('2017-12-01') - data['next_pymnt_d_date']).dt.days / 30.5)

# Menampilkan lima baris teratas dari kolom 'mths_since_next_pymnt_d'
data['mths_since_next_pymnt_d'].head()

# Menampilkan statistik deskriptif dari kolom 'mths_since_next_pymnt_d'
data['mths_since_next_pymnt_d'].describe()

# Menghapus kolom 'next_pymnt_d' dan 'next_pymnt_d_date'
data.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

"""##### Column: last_credit_pull_d"""

# Mengubah kolom 'last_credit_pull_d_date' menjadi format tanggal
data['last_credit_pull_d_date'] = pd.to_datetime(data['last_credit_pull_d'], format = '%b-%y')

# Menghitung jumlah bulan sejak 'last_credit_pull_d' hingga tanggal referensi
data['mths_since_last_credit_pull_d'] = round((pd.to_datetime('2017-12-01') - data['last_credit_pull_d_date']).dt.days / 30.5)

# Menampilkan lima baris teratas dari kolom 'mths_since_last_credit_pull_d'
data['mths_since_last_credit_pull_d'].head()

# Menampilkan statistik deskriptif dari kolom 'mths_since_last_credit_pull_d'
data['mths_since_last_credit_pull_d'].describe()

# Menghapus kolom 'last_credit_pull_d' dan 'last_credit_pull_d_date'
data.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

"""# Exploratory Data Analysis (EDA)

### Check correlation
"""

# Membuat heatmap untuk melihat hubungan korelasi antar variabel numerik

import seaborn as sns
import matplotlib.pyplot as plt

numeric_data = data.select_dtypes(include=[float, int])

correlation_matrix = numeric_data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', cbar=True)
plt.show()

"""### Check Cardinality Data

##### Categorical Data
"""

# Menampilkan jumlah nilai unik dari fitur-fitur dengan tipe data objek
data.select_dtypes(include='object').nunique()

# Menghapus kolom 'emp_title', 'title', dan 'application_type'
data.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)

"""##### Numerical Data"""

# Menampilkan jumlah nilai unik dari fitur-fitur dengan tipe data non-objek
data.select_dtypes(exclude='object').nunique()

# Menghapus kolom 'policy_code'
data.drop(['policy_code'], axis=1, inplace=True)

# Menampilkan persentase distribusi untuk setiap nilai unik di semua kolom kategorikal (object)
for column in data.select_dtypes(include='object').columns:
    print(f"Distribusi nilai unik pada kolom: {column}")
    distribution = data[column].value_counts(normalize=True) * 100
    print(distribution, end='\n\n')

# Menghapus kolom 'pymnt_plan'
data.drop('pymnt_plan', axis=1, inplace=True)

"""### Univariate Analysis

##### Categorical Data
"""

# Mengidentifikasi dan menyimpan nama-nama kolom kategorikal
cat_var = data.select_dtypes(include=["object"]).columns
cat_var

import seaborn as sns
import matplotlib.pyplot as plt

# Loop melalui setiap nama kolom di dalam list cat_var
for column in cat_var:
    plt.figure(figsize=(10, 6))

    sns.countplot(data=data, y=column, order=data[column].value_counts().index)

    plt.title(f'Distribusi Variabel: {column}', fontsize=15)
    plt.xlabel('Jumlah', fontsize=12)
    plt.ylabel(column, fontsize=12)
    plt.show()

"""##### Numerical Data"""

# Mengidentifikasi dan menyimpan nama-nama kolom numerik
num_var = data.select_dtypes(exclude='object').columns
num_var

import seaborn as sns
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# Loop melalui setiap nama kolom di dalam list num_var
for column in num_var:
    mean_val = data[column].mean()
    median_val = data[column].median()
    mode_val = data[column].mode().values[0]

    plt.figure(figsize=(12, 6))
    sns.histplot(data[column], kde=True, bins=40)

    plt.axvline(mean_val, color='red', linestyle='solid', linewidth=2, label=f'Mean: {mean_val:.2f}')
    plt.axvline(median_val, color='yellow', linestyle='dotted', linewidth=3, label=f'Median: {median_val:.2f}')
    plt.axvline(mode_val, color='purple', linestyle='dashed', linewidth=2, label=f'Mode: {mode_val:.2f}')

    plt.title(f'Distribusi Variabel: {column}', fontsize=15)
    plt.legend()
    plt.show()

"""### Bivariate Analysis"""

plt.style.use("ggplot")
for column in cat_var:
    plt.figure(figsize=(20, 4))
    plt.subplot(121)
    sns.countplot(x=data[column], hue=data["bad_flag"])
    plt.title(column)
    plt.xticks(rotation=90)

"""# Data Preprocessing

### Check Missing Values
"""

# Menampilkan presentase data yang hilang
check_missing = data.isnull().sum() * 100 / data.shape[0]
missing_values = check_missing[check_missing > 0].sort_values(ascending=False)
print(missing_values)

# Menghapus kolom 'mths_since_last_record'
data.drop('mths_since_last_record', axis=1, inplace=True)

"""### Data Imputation"""

# Mengecek jumlah NaN di setiap kolom
data.isnull().sum()

# Melakukan imputasi terhadap kolom yang memiliki nilai NaN

data['annual_inc'].fillna(data['annual_inc'].median(), inplace=True)
data['mths_since_earliest_cr_line'].fillna(0, inplace=True)
data['acc_now_delinq'].fillna(0, inplace=True)
data['total_acc'].fillna(0, inplace=True)
data['pub_rec'].fillna(0, inplace=True)
data['open_acc'].fillna(0, inplace=True)
data['inq_last_6mths'].fillna(0, inplace=True)
data['delinq_2yrs'].fillna(0, inplace=True)
data['collections_12_mths_ex_med'].fillna(0, inplace=True)
data['revol_util'].fillna(0, inplace=True)
data['emp_length_int'].fillna(0, inplace=True)
data['mths_since_last_delinq'].fillna(-1, inplace=True)
data['grade'].fillna('N/A', inplace=True)
data['home_ownership'].fillna('UNKNOWN', inplace=True)
data['verification_status'].fillna('UNKNOWN', inplace=True)
data['purpose'].fillna('UNKNOWN', inplace=True)
data['addr_state'].fillna('UNKNOWN', inplace=True)
data['dti'].fillna(data['dti'].median(), inplace=True)
data['term_int'].fillna(0, inplace=True)
data['mths_since_issue_d'].fillna(0, inplace=True)
data['mths_since_last_pymnt_d'].fillna(-1, inplace=True)
data['mths_since_next_pymnt_d'].fillna(-1, inplace=True)
data['mths_since_last_credit_pull_d'].fillna(0, inplace=True)

"""### Label Encoding"""

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()

for column in cat_var:
  data[column] = label.fit_transform(data[column])

"""### Handling Imbalanced Data"""

# Menangani imbalance data dengan random oversampling

from imblearn.over_sampling import RandomOverSampler

X = data.drop('bad_flag', axis=1)
y = data['bad_flag']

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Mengecek distribusi kelas setelah resampling
y_resampled.value_counts()

"""### Train Test Split"""

from sklearn.model_selection import train_test_split

# Membagi train set dan test set dengan proporsi 80:20
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

print("Jumlah data latih:", len(X_train))
print("Jumlah data uji:", len(X_test))

"""### Standardization"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Menampilkan lima baris teratas dari X_train
X_train_scaled.head()

# Menampilkan lima baris teratas dari X_test
X_test_scaled.head()

"""# Data Modelling

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

logreg_model = LogisticRegression()
logreg_model.fit(X_train_scaled, y_train)

"""### ROC Curve"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_prob_train_logreg = logreg_model.predict_proba(X_train_scaled)[:, 1]
y_prob_test_logreg = logreg_model.predict_proba(X_test_scaled)[:, 1]

fpr_train, tpr_train, _ = roc_curve(y_train, y_prob_train_logreg)
roc_auc_train = roc_auc_score(y_train, y_prob_train_logreg)

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_logreg)
roc_auc_test = roc_auc_score(y_test, y_prob_test_logreg)

plt.figure(figsize=(6, 5))
plt.plot(fpr_train, tpr_train, color='cyan', lw=2, label=f'Train ROC Curve (AUC = {roc_auc_train:.2f})')
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'Test ROC Curve (AUC = {roc_auc_test:.2f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: Train vs Test')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

"""### Confusion Matrix"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

y_test_pred_logreg = logreg_model.predict(X_test_scaled)

cm_logreg = confusion_matrix(y_test, y_test_pred_logreg)
print("Confusion Matrix for Logistic Regression:")
print(cm_logreg)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_logreg, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix: Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

print("\nClassification Report for Logistic Regression:")
print(classification_report(y_test, y_test_pred_logreg))

"""### LightGBM"""

import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=100, max_depth=5, learning_rate=0.1)
lgb_model.fit(X_train_scaled, y_train)

"""### ROC Curve"""

y_prob_train_lgb = lgb_model.predict_proba(X_train_scaled)[:, 1]
y_prob_test_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]

fpr_train_lgb, tpr_train_lgb, _ = roc_curve(y_train, y_prob_train_lgb)
roc_auc_train_lgb = roc_auc_score(y_train, y_prob_train_lgb)

fpr_test_lgb, tpr_test_lgb, _ = roc_curve(y_test, y_prob_test_lgb)
roc_auc_test_lgb = roc_auc_score(y_test, y_prob_test_lgb)

plt.figure(figsize=(6, 5))
plt.plot(fpr_train_lgb, tpr_train_lgb, color='cyan', lw=2, label=f'Train ROC Curve (AUC = {roc_auc_train_lgb:.2f})')
plt.plot(fpr_test_lgb, tpr_test_lgb, color='blue', lw=2, label=f'Test ROC Curve (AUC = {roc_auc_test_lgb:.2f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve LightGBM: Train vs Test')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

"""### Confusion Matrix"""

y_train_pred_lgb = lgb_model.predict(X_train_scaled)
y_test_pred_lgb = lgb_model.predict(X_test_scaled)

cm_lgb = confusion_matrix(y_test, y_test_pred_lgb)
print("Confusion Matrix for LightGBM:")
print(cm_lgb)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_lgb, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix: LightGBM')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

print("\nClassification Report for LightGBM:")
print(classification_report(y_test, y_test_pred_lgb))

"""### XGBoost"""

import xgboost as xgb

xgb_model = xgb.XGBClassifier(random_state=42, n_estimators=100, max_depth=5, learning_rate=0.1)
xgb_model.fit(X_train_scaled, y_train)

"""### ROC Curve"""

y_prob_train_xgb = xgb_model.predict_proba(X_train_scaled)[:, 1]
y_prob_test_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]

fpr_train_xgb, tpr_train_xgb, _ = roc_curve(y_train, y_prob_train_xgb)
roc_auc_train_xgb = roc_auc_score(y_train, y_prob_train_xgb)

fpr_test_xgb, tpr_test_xgb, _ = roc_curve(y_test, y_prob_test_xgb)
roc_auc_test_xgb = roc_auc_score(y_test, y_prob_test_xgb)

plt.figure(figsize=(6, 5))
plt.plot(fpr_train_xgb, tpr_train_xgb, color='cyan', lw=2, label=f'Train ROC Curve (AUC = {roc_auc_train_xgb:.2f})')
plt.plot(fpr_test_xgb, tpr_test_xgb, color='blue', lw=2, label=f'Test ROC Curve (AUC = {roc_auc_test_xgb:.2f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve XGBoost: Train vs Test')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

"""### Confusion Matrix"""

y_train_pred_xgb = xgb_model.predict(X_train_scaled)
y_test_pred_xgb = xgb_model.predict(X_test_scaled)

cm_xgb = confusion_matrix(y_test, y_test_pred_xgb)
print("Confusion Matrix for XGBoost:")
print(cm_xgb)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix: XGBoost')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

print("\nClassification Report for XGBoost:")
print(classification_report(y_test, y_test_pred_xgb))